{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torch import embedding\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import random\n",
    "from random import shuffle\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject():\n",
    "    def __init__(self,n):\n",
    "        self.subject_number = n\n",
    "        global df\n",
    "        df = pd.read_csv(\"data/Text/{0:0>3n}(done).csv\".format(n))\n",
    "        self.words = df[\"Words\"]\n",
    "        self.start_time = df[\"Start_time\"]\n",
    "        self.shuffled_words_list = []  #已經經過shuffled的word list\n",
    "        for i in range(len(self.words)):\n",
    "            self.shuffled_words_list.append(self.words[i])\n",
    "        shuffle(self.shuffled_words_list)\n",
    "        for i in range(len(self.shuffled_words_list)):\n",
    "            df[\"Words\"][i] = self.shuffled_words_list[i]\n",
    "        #df.to_csv(f'data/Text/{n:0>3d}(done_shuffled).csv')\n",
    "    ##########################################################################################\n",
    "    def shuffled_calculate(self,destination,timeframe,space,start_time=0,end=480):\n",
    "        global df\n",
    "        df['Start_time'] = pd.to_numeric(df['Start_time'])\n",
    "        #df['end_time'] = pd.to_numeric(df['end_time'])\n",
    "        global all_sentences001\n",
    "        all_sentences001 = []\n",
    "        t=start_time\n",
    "        \n",
    "        while t<end:\n",
    "            sentences = []\n",
    "            for index, rows in df.loc[(df[\"Start_time\"]>=t) & (df[\"Start_time\"]<=(t+timeframe))].iterrows():\n",
    "                sentences.append(rows[\"Words\"])\n",
    "            all_sentences001.append(\" \".join(sentences))\n",
    "            t+=space\n",
    "\n",
    "        \n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        global SBERT \n",
    "        SBERT= []\n",
    "\n",
    "        def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "            return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n",
    "        # Two lists of sentences\n",
    "        for i in range(0,len(all_sentences001)-1):\n",
    "            #Compute embedding for both lists\n",
    "            embeddings1 = model.encode(all_sentences001[i], convert_to_tensor=True)\n",
    "            #print(all_sentences001[i])\n",
    "            embeddings2 = model.encode(all_sentences001[i+1], convert_to_tensor=True)\n",
    "            #print(all_sentences001[i+1])\n",
    "            #Compute cosine-similarities\n",
    "            cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "            #print(cosine_scores)\n",
    "            # convert from PyTorch tensor to numpy array\n",
    "            SBERT.append(get_cosine_similarity(embeddings1, embeddings2))\n",
    "        ######\n",
    "            print(\"timeframe: \",i,\":\",i+10,\"\\n\")\n",
    "            print(get_cosine_similarity(embeddings1, embeddings2),\"\\n\")\n",
    "            print(all_sentences001[i],\"\\n\")\n",
    "            print(all_sentences001[i+1],\"\\n\")\n",
    "            print()\n",
    "\n",
    "        \n",
    "        all_sentences001 = []\n",
    "        stops = set(stopwords.words('english'))\n",
    "        t=start_time\n",
    "        while t <end:\n",
    "            sentences = []\n",
    "            for index, rows in df.loc[(df[\"Start_time\"]>=t) & (df[\"Start_time\"]<=(t+timeframe))].iterrows():\n",
    "                if rows[\"Words\"].lower() not in stops:\n",
    "                    sentences.append(rows[\"Words\"].lower())\n",
    "                    #print(rows[\"Words\"])\n",
    "            all_sentences001.append(\" \".join(sentences))\n",
    "            #print(t)\n",
    "            t += space \n",
    "        ''' 舊的sample step\n",
    "        for t in range(0,480,0.5):\n",
    "            sentences = []\n",
    "            for index, rows in df.loc[(df[\"Start_time\"]>=t) & (df[\"Start_time\"]<=(t+10))].iterrows():\n",
    "                if rows[\"Words\"].lower() not in stops:\n",
    "                    sentences.append(rows[\"Words\"].lower())\n",
    "            all_sentences001.append(\" \".join(sentences))\n",
    "        '''    \n",
    "        #nltk.download('omw-1.4')\n",
    "        #nltk.download('wordnet')\n",
    "        #nltk.download('stopwords')\n",
    "        global pair_counts\n",
    "        pair_counts = []\n",
    "        stops = set(stopwords.words('english'))\n",
    "        global all_sentences_wu_sum\n",
    "        all_sentences_wu_sum = []\n",
    "        global all_sentences_wu_sum_divided\n",
    "        all_sentences_wu_sum_divided = []\n",
    "        for sentence_num in range(0,len(all_sentences001)): #現在第幾句\n",
    "            wu_pulmer_sum = 0\n",
    "            the_sentence = str(all_sentences001[sentence_num]).split()\n",
    "            #print(the_sentence)\n",
    "            timeframe_pairs_count = 0\n",
    "            for words_num in range(0, (len(the_sentence)-1)): #標定第一個字\n",
    "                if len(wordnet.synsets(the_sentence[words_num])) > 0 :\n",
    "                    syns1 = wordnet.synsets(the_sentence[words_num])\n",
    "                    syns1_num = len(syns1)\n",
    "                else:\n",
    "                    continue\n",
    "                for words_num2 in range((words_num+1), len(the_sentence)): #標定第二個字\n",
    "                    if len(wordnet.synsets(the_sentence[words_num2])) > 0 :\n",
    "                        syns2 = wordnet.synsets(the_sentence[words_num2])\n",
    "                        syns2_num = len(syns2)\n",
    "                        for i in range(0, syns1_num):\n",
    "                            for u in range(0, syns2_num):\n",
    "                                #print(the_sentence[words_num],\"的\",syns1[i],\" \", the_sentence[words_num2],\"的\",syns2[u],\" \" ,syns1[i].wup_similarity(syns2[u]))\n",
    "                                wu_pulmer_sum += syns1[i].wup_similarity(syns2[u])\n",
    "                                timeframe_pairs_count += 1\n",
    "                    else:\n",
    "                        continue\n",
    "            if timeframe_pairs_count != 0:\n",
    "                all_sentences_wu_sum_divided.append(wu_pulmer_sum/timeframe_pairs_count) #算出除以pairs數量的平均值\n",
    "            else: \n",
    "                all_sentences_wu_sum_divided.append(wu_pulmer_sum)\n",
    "            all_sentences_wu_sum.append(wu_pulmer_sum)\n",
    "            pair_counts.append(timeframe_pairs_count)\n",
    "            #print(all_sentences_wu_sum)\n",
    "            global wu_difference\n",
    "            wu_difference = []\n",
    "            for x in range(len(all_sentences_wu_sum)-1):\n",
    "                wu_difference.append(all_sentences_wu_sum[x+1]-all_sentences_wu_sum[x])\n",
    "        global results_csv\n",
    "        results_csv = pd.DataFrame(list(zip(SBERT,all_sentences_wu_sum,all_sentences_wu_sum_divided,pair_counts)), columns = ['SBERT',\"Wu-Pulmer\",\"Wu-Pulmer_divided\",\"pair_counts\"])\n",
    "        #results_csv.to_csv(\"data/Text/{0:0>3d}({1}).csv\".format(self.subject_number,destination))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意出來的self.words已經是自動打亂的結果了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\AppData\\Local\\Temp\\ipykernel_37024\\757665209.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Words\"][i] = self.shuffled_words_list[i]\n"
     ]
    }
   ],
   "source": [
    "subject8 = Subject(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          the\n",
       "1       stupid\n",
       "2        where\n",
       "3            I\n",
       "4       always\n",
       "         ...  \n",
       "1464      yeah\n",
       "1465     about\n",
       "1466       not\n",
       "1467       you\n",
       "1468       cuz\n",
       "Name: Words, Length: 1469, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject8.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.words就是df[\"Words\"]\n",
    "self.start_time = 所有字的starttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_word import RandomWords\n",
    "r = RandomWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\AppData\\Local\\Temp\\ipykernel_37024\\3765328463.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subject8.words[i] = r.get_random_word()\n"
     ]
    }
   ],
   "source": [
    "random_words_list = []\n",
    "for i in range(0, len(subject8.words)):\n",
    "    random_words_list.append(r.get_random_word())\n",
    "    subject8.words[i] = r.get_random_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     disgradulate\n",
      "1          railway\n",
      "2      atomistical\n",
      "3    wanderingness\n",
      "4      evaluations\n",
      "Name: Words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(subject8.words.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def shuffled_calculate(self,destination,timeframe,space,start_time=0,end=480):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subject8.shuffled_calculate(\"shuffled\",10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'data/Text/008(done_random_words3).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject8.shuffled_calculate(\"random_words\",10,1)\\nglobal results_csv\\nresults_csv.to_csv(\"data/Text/{0:0>3d}({1}).csv\".format(8,\"results_random_words2\"))'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''subject8.shuffled_calculate(\"random_words\",10,1)\n",
    "global results_csv\n",
    "results_csv.to_csv(\"data/Text/{0:0>3d}({1}).csv\".format(8,\"results_random_words2\"))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('semantic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c54165d08fd550b20e5d70a45269718a12194b1a3ed2e8c4cf823acd6cf23912"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
